{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process Demo\n",
    "\n",
    "This notebook briefly describes how to make an variational inference with Henbun.\n",
    "\n",
    "*Keisuke Fujii, 21st Nov. 2016*\n",
    "\n",
    "We show \n",
    "+ Simple Gaussian Process regression\n",
    "+ Regression with non-Gaussian likelihood where ananlytical posterior is not available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import Henbun as hb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# random state\n",
    "rng = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0,6,40).reshape(-1,1)\n",
    "Y = np.sin(X) + rng.randn(40,1)*0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(X, Y, 'o', label='data')\n",
    "plt.plot(X, np.sin(X), '--k', label='true')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a Henbun model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Any model should inherite hb.model.Model\n",
    "class GPR(hb.model.Model):\n",
    "    def setUp(self):\n",
    "        \"\"\" \n",
    "        Set up parameters and Data for this model.\n",
    "        Model.setUp is immediately called after Model.__init__()\n",
    "        \"\"\"\n",
    "        # Data should be stored in hb.param.Data class. \n",
    "        self.X = hb.param.Data(X)\n",
    "        self.Y = hb.param.Data(Y)\n",
    "        \n",
    "        # Variational parameters. \n",
    "        # By default, the shape of the variational covariance is 'diagonal'.\n",
    "        # In small problems, the fullrank covariance can be inferred.\n",
    "        self.q = hb.variationals.Gaussian(shape=X.shape, q_shape='fullrank')\n",
    "        \n",
    "        # Kernel object for GPR. \n",
    "        self.kern = hb.gp.kernels.UnitRBF()\n",
    "        # Since our kernel does not contain the variance term, \n",
    "        # we should multiply positive parameter by hand.\n",
    "        # To constrain k_var to stay in positive space, set transform option.\n",
    "        self.k_var = hb.param.Variable(shape=[1], transform=hb.transforms.positive)\n",
    "        \n",
    "        # likelihood variance, which is also positive parameter.\n",
    "        self.var = hb.param.Variable(shape=[1], transform=hb.transforms.positive)\n",
    "        \n",
    "    @hb.model.AutoOptimize()\n",
    "    def ELBO_gaussian(self):\n",
    "        \"\"\"\n",
    "        Any method decorated by @AutoOptimize can be optimized.\n",
    "        In the decorated method, [hb.param.Variable, hb.variationals.Variational, \n",
    "        hb.param.Data] objects are treated as tf.Tensor.\n",
    "        Therefore, tensorflow's method can be directoly used.\n",
    "        \n",
    "        Here, we calculate ELBO that should be maximized.\n",
    "        \"\"\"\n",
    "        y_fit = tf.matmul(self.kern.Cholesky(self.X), self.q) * tf.sqrt(self.k_var)\n",
    "        \n",
    "        # Kulback-Leibler divergence can be accessed by self.KL() method.\n",
    "        return tf.reduce_sum(hb.densities.gaussian(self.Y, y_fit, self.var))\\\n",
    "                - self.KL()\n",
    "        \n",
    "    @hb.model.AutoOptimize()\n",
    "    def ELBO_student(self):\n",
    "        \"\"\"\n",
    "        It is often convenient to prepare several variants of target method.\n",
    "        In this method, we assume Student's t distribution as likelihood.\n",
    "        \"\"\"\n",
    "        y_fit = tf.matmul(self.kern.Cholesky(self.X), self.q) * tf.sqrt(self.k_var)\n",
    "        # Kulback-Leibler divergence can be accessed by self.KL() method.\n",
    "        return tf.reduce_sum(hb.densities.student_t(self.Y, y_fit, self.var, 3.0))\\\n",
    "                - self.KL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_gpr = GPR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Henbun model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, we need a compilation of the model\n",
    "model_gpr.ELBO_gaussian().compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To evaluate this method with current parameters, run() method can be used.\n",
    "model_gpr.ELBO_gaussian().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To optimize this method, optimize() method can be used.\n",
    "model_gpr.ELBO_gaussian().optimize(maxiter=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# See the resultant ELBO value\n",
    "model_gpr.ELBO_gaussian().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There are some options to obtain the result.\n",
    "# Most primitive way is to make an op and then run by model.run() method with appropriate feed_dict.\n",
    "with model_gpr.tf_mode():\n",
    "    op = tf.matmul(model_gpr.kern.Cholesky(model_gpr.X), model_gpr.q) * tf.sqrt(model_gpr.k_var)\n",
    "\n",
    "# In each run, the different samples are taken from the variational parameters\n",
    "f_samples = [model_gpr.run(op) for _ in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(X, Y, 'o', label='data')\n",
    "for s in f_samples: plt.plot(X, s,'k', alpha=0.2)\n",
    "plt.plot(X, np.sin(X), '--k', label='true')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the optimized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To get the current parameters, .value property can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('kernel lengthscale', model_gpr.kern.lengthscales.value)\n",
    "print('kernel variance',    model_gpr.k_var.value)\n",
    "print('likelihood variance',model_gpr.var.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Gaussian likelihood model\n",
    "In this example, we demonstrate the use of non-gaussian likelihood in Gaussian Process Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We added some outliers to the above Toy dat\n",
    "Y[np.random.randint(0,X.shape[0],5),0] = 2*rng.randn(5) # Add non-Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(X, Y, 'o', label='data')\n",
    "plt.plot(X, np.sin(X), '--k', label='true')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace the data of the existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just assign new values\n",
    "model_gpr.X = X\n",
    "model_gpr.Y = Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Gaussian likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_gpr.ELBO_gaussian().compile()\n",
    "model_gpr.ELBO_gaussian().optimize(maxiter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_samples = [model_gpr.run(op) for _ in range(30)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Student's likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To optimize this method, optimize() method can be used.\n",
    "model_gpr.ELBO_student().compile()\n",
    "model_gpr.ELBO_student().optimize(maxiter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_samples_student = [model_gpr.run(op) for _ in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(X, Y, 'o', label='data')\n",
    "for s in f_samples: plt.plot(X, s,'k', alpha=0.2)\n",
    "for s in f_samples_student: plt.plot(X, s,'r', alpha=0.2)\n",
    "plt.plot(X, np.sin(X), '--k', label='true')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "\n",
    "The above is an simple example of Henbun usage.\n",
    "\n",
    "However, Henbun is not very efficient for such a simple problem.  \n",
    "Another single-purpose library such as [GPflow](https://github.com/GPflow/GPflow/blob/master/GPflow/likelihoods.py) is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
